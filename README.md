# Llama-inference-jax

Accelerated inference with Llama Models in JAX for high-speed, pure JAX implementation.

> [!NOTE]
>
> This project will only support Llama Models (at least for now), and focuses on local machines
> so if you are more likely trying to use this project for any other purposes I suggest you check
> out [EasyDeL](https://github.com/erfanzar/EasyDeL).

## Overview

Llama-inference-jax is a library designed to perform accelerated inference using Llama Models in JAX, providing
high-speed and pure JAX implementation. Llama Models are known for their efficiency and accuracy in various machine
learning tasks, and integrating them with JAX allows for seamless deployment on accelerators like GPUs and TPUs.

## Features

- Accelerated inference with Llama Models.
- Pure JAX implementation for high-speed execution.
- Seamless deployment on GPUs and TPUs.
- Custom Pallas Kernels.
- Parameter Quantization.
- Standalone weights.
- Flash Attention Support on CPU/GPU/TPU.

## Usage

_Process is still under progress ..._

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.